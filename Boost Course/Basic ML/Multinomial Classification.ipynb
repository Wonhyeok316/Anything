{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[-0.62458473,  0.4234396 , -1.0987784 ],\n",
       "        [ 0.20714556,  1.6756382 , -0.45219132],\n",
       "        [ 0.42504662,  0.68134964, -1.1231694 ],\n",
       "        [-0.7173501 ,  1.0393274 , -0.6222199 ]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.48150456, -0.9141008 , -0.06468647], dtype=float32)>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "# 8 * 4\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# 입력 데이터를 ndarray로 변환하지만, 이미 ndarray일 경우에는\n",
    "# 메모리에 새로 ndarray를 생성하지는 않는다.\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "nb_classes = 3\n",
    "\n",
    "# 먼저 Weight와 bias setting\n",
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name=\n",
    "                'weight')\n",
    "# 4 * 3\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.9261124e-03 9.8985004e-01 2.2380595e-04]\n",
      " [1.5761345e-03 9.9842179e-01 2.1263938e-06]\n",
      " [1.6492640e-05 9.9998355e-01 1.6750594e-08]\n",
      " [5.9787271e-07 9.9999940e-01 1.8791416e-11]\n",
      " [2.0678201e-09 1.0000000e+00 5.1594750e-15]\n",
      " [5.5130062e-07 9.9999940e-01 4.0881559e-11]\n",
      " [1.1996237e-09 1.0000000e+00 1.3533690e-15]\n",
      " [3.6902870e-11 1.0000000e+00 5.0352014e-18]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "# 8 * 3\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3.3605112e-08 1.0000000e+00 3.6463898e-11]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 점검\n",
    "sample_db = [[8, 2, 1, 4]]\n",
    "sample_db = np.array(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.49183, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(hypothesis(X)), axis=1)\n",
    "    # axis=1이면 행단위로 sum\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "# tf.Tensor\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x*x\n",
    "dy_dx = g.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=301515, shape=(4, 3), dtype=float32, numpy=\n",
      "array([[-0.24835865,  0.9983302 , -0.7499715 ],\n",
      "       [-1.6223192 ,  2.122263  , -0.4999438 ],\n",
      "       [-1.6231613 ,  2.4981325 , -0.8749713 ],\n",
      "       [-1.6233561 ,  2.4983277 , -0.8749715 ]], dtype=float32)>, <tf.Tensor: id=301514, shape=(3,), dtype=float32, numpy=array([-0.24856001,  0.6235318 , -0.37497175], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Gradient 계산\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        \n",
    "        return grads\n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 100 : 0.611678\n",
      "Loss at epoch 200 : 0.528341\n",
      "Loss at epoch 300 : 0.476533\n",
      "Loss at epoch 400 : 0.434171\n",
      "Loss at epoch 500 : 0.396086\n",
      "Loss at epoch 600 : 0.359963\n",
      "Loss at epoch 700 : 0.324296\n",
      "Loss at epoch 800 : 0.288052\n",
      "Loss at epoch 900 : 0.252435\n",
      "Loss at epoch 1000 : 0.231716\n",
      "Loss at epoch 1100 : 0.220287\n",
      "Loss at epoch 1200 : 0.209932\n",
      "Loss at epoch 1300 : 0.200481\n",
      "Loss at epoch 1400 : 0.191820\n",
      "Loss at epoch 1500 : 0.183854\n",
      "Loss at epoch 1600 : 0.176504\n",
      "Loss at epoch 1700 : 0.169701\n",
      "Loss at epoch 1800 : 0.163387\n",
      "Loss at epoch 1900 : 0.157512\n",
      "Loss at epoch 2000 : 0.152032\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs = 2000, verbose=100):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        # variables = [W, b]\n",
    "        if (i==0) | (i+1)%verbose==0:\n",
    "            print('Loss at epoch %d : %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00180109 0.07805449 0.92014444]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2, 1, 3, 2]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "print(tf.argmax(a, 1))\n",
    "# tf.argmax는 가장 큰 값의 인덱스를 반환. 1은 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5.19746209e-06 1.35833642e-03 9.98636425e-01]\n",
      " [1.80109218e-03 7.80545101e-02 9.20144439e-01]\n",
      " [2.10684927e-08 1.57926470e-01 8.42073500e-01]\n",
      " [4.18982722e-07 8.56208861e-01 1.43790722e-01]\n",
      " [2.50895321e-01 7.37412333e-01 1.16923684e-02]\n",
      " [1.31969541e-01 8.67921710e-01 1.08736625e-04]\n",
      " [7.61733532e-01 2.38210127e-01 5.63513750e-05]\n",
      " [9.17484820e-01 8.25140029e-02 1.12362295e-06]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1))\n",
    "#맞는 것도 있고, 틀린 것도 있고.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
